{
    "https://medium.com/@avra42/streamlit-python-cool-tricks-to-make-your-web-application-look-better-8abfc3763a5b": {
        "author": "Avra",
        "length": "4 min read",
        "title": "Streamlit Python Cool Tricks to make Your Web-Application look better",
        "description": "Probably you have often wondered, what can be the better ways to make your Streamlit web application shine and be user-friendly. This article covers a few easy steps, that can minimalize and tweak the already added Streamlit features within the default web application.",
        "tags": [
            "streamlit",
            "python",
            "python-web-developer",
            "data-visualization",
            "data-science"
        ],
        "content": [
            "Default radio button widgets come in form of columns (vertically arranged). At times, we may need to convert them into rows. This can be very well achieved with one line of code. But first, let's deploy a very simple Streamlit web application with radio buttons on it.",
            "# Import streamlit as st st.header(\"Cool Streamlit Tricks\") # Create Radio Buttons st.radio(label = 'Radio buttons', options = ['R1','R2','R3']",
            "When we run the above code snippet on our terminal, the application looks something like this (below the screen grasp).",
            "Now, we aim to convert the vertically placed radio widgets into row form. For that, we need this additional piece of line,",
            "st.write('<style>div.row-widget.stRadio > div{flex-direction:row;}</style>', unsafe_allow_html=True)",
            "Great! We achieved what we aimed for!",
            "What can be more attractive, than choosing the right color theme? In Streamlit this can be easily altered using the Settings options that pop up on clicking the Main-menu(three dashes) option, placed on the top right-hand side of the deployed web application. Let me guide you through the process step by step.",
            "So, choose your best color combination (or Font Family) and press the Copy theme to clipboard option.",
            "In order to use your favorite color theme on every run of your web application,",
            "Create a .streamlit folder within your folder containing the python script for your web application (for instance, my python script file is named as main.py ) Create a config.toml file inside the folder Paste the copied content within the config.toml file",
            "The content within the config.toml looks something like this.",
            "Highlighting texts is possibly the best way to grab your viewer's attention. This Streamlit component enables us to perform this cool feature. Below is the link for the GitHub repository.",
            "tvst/st-annotated-text: A simple component to display annotated text in Streamlit apps. (github.com)",
            "How can we use it? Below is the code snippet.",
            "By default, Streamlit web application takes the name of the python script. This can be very easily customized using the Sreamlit function \u2014 set_page_config . Alongside, page_title as one of the inputs, it also takes page_icon as one of the other arguments, thereby enabling us to modify the default Streamlit icon.",
            "With few additional lines of code (below), our web application now consists of a user-defined name (i.e 'Streamlit Tricks') and an icon.",
            "We may require to remove the default features, such as the Main-menu(three dashes at the top right-hand corner) and also the footer note ('Made with Streamlit').",
            "The code below can be used to perform the removal/hiding task.",
            "hide_menu_style = \"\"\" <style> #MainMenu {visibility: hidden; } footer {visibility: hidden;} </style> \"\"\" st.markdown(hide_menu_style, unsafe_allow_html=True)",
            "Easy visualization of large data frames or data tables, makes the complicated process of data sorting or data manipulation much easier and user-friendly. So, let's try to impart some highlights within our Streamlit data frame.",
            "import streamlit as st import pandas as pd # Function def color_df(val): if val > 21: color = 'green' else : color = 'red' return f'background-color: {color}' # Our example Dataset data = [['Tom', 23], ['Nick', 18], ['Bob', 20], ['Martin', 25]] # Create Pandas DataFrame df = pd.DataFrame(data, columns = ['Name', 'Age']) # Using Style for the Dataframe st.dataframe(df.style.applymap(color_df, subset=['Age']))",
            "I have also prepared a video, describing all the above steps in detail. The link for the video :"
        ]
    },
    "https://laurajedeed.medium.com/afghanistan-meant-nothing-9e3f099b00e5": {
        "author": "Laura Jedeed",
        "length": "4 min read",
        "title": "Afghanistan Meant Nothing",
        "description": "",
        "tags": [
            "afghanistan",
            "afghanistan-war",
            "war-on-terror",
            "taliban",
            "biden"
        ],
        "content": [
            "A Veteran Reflects on 20 Wasted Years",
            "By the time you read this, the Taliban may already be in Kabul. If not now, then soon.",
            "Nixon wanted \u2014 and got \u2014 his decent interval between the United States pullout of Vietnam and the inevitable North Vietnamese takeover. Afghanistan\u2019s interval was never going to be decent, but I confess I expected an interval. We\u2019re scrambling to leave in time, we\u2019re racing for the helicopters as the Taliban burns through Afghanistan like a forest fire.",
            "I remember Afghanistan well. I deployed there twice \u2014 once in 2008, and again in 2009\u20132010. It was already obvious that the Taliban would sweep through the very instant we left. And here we are today.",
            "I know how bad the Taliban is. I know what they do to women and little boys. I know what they\u2019re going to do to the interpreters and the people who cooperated with us, it\u2019s awful, it\u2019s bad, but we are leaving, and all I feel is grim relief.",
            "This is what I remember:",
            "I remember Afghanistan as a dusty beige nightmare of a place full of proud, brave people who did not fucking want us there. We called them Hajjis and worse and they were better than we were, braver and stronger and smarter.",
            "I remember going through the phones of the people we detained and finding clip after clip of Bollywood musicals, women singing in fields of flowers. Rarely did I find anything incriminating.",
            "I remember finding propaganda footage cut together from the Soviet invasion and our own Operation Enduring Whatever. I remember laughing about how stupid the Afghans were to not know we aren\u2019t the Russians and then, eventually, realizing that I was the stupid one.",
            "I remember how every year the US would have to decide how to deal with the opium fields. There were a few options. You could leave the fields alone, and then the Taliban would shake the farmers down and use the money to buy weapons. Or, you could carpet bomb the fields, and then the farmers would join the Taliban for reasons that, to me, seem obvious.",
            "The third option, and the one we went for while I was there, was to give the farmers fertilizer as an incentive to grow wheat instead of opium poppy. The farmers then sold the fertilizer to the Taliban, who used it to make explosives for IEDs that could destroy a million dollar MRAP and maim everyone inside.",
            "I remember we weren\u2019t allowed to throw batteries away because people who worked on base would go through the trash and collect hundreds of dead batteries, wire them together so they had just enough juice for one charge, and use that charge to detonate an IED.",
            "I remember the look on my roommate\u2019s face after she got back from cutting the dead bodies of two soldiers out of an HMMWV that got blown up by an IED that I have always imagined was made with fertilizer from an opium farmer and detonated with a hundred thrown-out batteries.",
            "I remember an Afghan kid who worked in the DFAC (cafeteria) who we called Cowboy. He always wore this cowboy hat and an \u201cI\u2019m with stupid\u201d t-shirt someone had given him, always with a big smile, high school age.",
            "Cowboy was a good student. His family, who all worked on base, was incredibly proud of him. He wanted to go to college in America. But there weren\u2019t colleges that took Afghans, the education system was too shit. No program to help kids like him. I looked.",
            "I wonder if he\u2019s dead now, for serving us food and dreaming of something different.",
            "But if Cowboy is dead then he died a long time ago, and if Cowboy is dead it\u2019s our fault for going there in the first place, giving his family the option of trusting us when we are the least trustworthy people on the planet.",
            "We use people up and throw them away like it\u2019s nothing.",
            "And now, finally, we are leaving and the predictable thing is happening. The Taliban is surging in and taking it all back. They were always going to do this, because they have a thing you cannot buy or train, they have patience and a bloody-mindedness that warrants more respect than we ever gave them.",
            "I am Team Get The Fuck Out Of Afghanistan which, as a friend pointed out to me today, has always been Team Taliban. It\u2019s Team Taliban or Team Stay Forever.",
            "There is no third team.",
            "And so I sit here, reading these sad fucking articles and these horrified social media posts about the suffering in Afghanistan and the horror of the encroaching Taliban and how awful it is that this is happening but I can\u2019t stop feeling this grim happiness, like, finally, you fuckers, finally you have to face the thing Afghanistan has always been. You can\u2019t keep lying to yourself about what you sent us into.",
            "No more blown up soldiers. No more Bollywood videos on phones whose owners are getting shipped god knows where. No more hypocrisy.",
            "No more pretending it meant anything. It didn\u2019t.",
            "It didn\u2019t mean a goddamn thing."
        ]
    },
    "https://towardsdatascience.com/improve-linear-regression-for-time-series-forecasting-e36f3c3e3534": {
        "author": "Marco Cerliani",
        "length": "4 min read",
        "title": "Improve Linear Regression for Time Series Forecasting",
        "description": "Combine Linear Models and Decision Trees for better Forecasting",
        "tags": [
            "data-science",
            "machine-learning",
            "artificial-intelligence",
            "time-series-forecasting",
            "linear-regression"
        ],
        "content": [
            "Time series forecasting is a very fascinating task. However, build a machine-learning algorithm to predict future data is trickier than expected. The hardest thing to handle is the temporal dependency present in the data. By their nature, time-series data are subject to shifts. This may result in temporal drifts of various kinds which may become our algorithm inaccurate.",
            "One of the best tips I recommend, when modeling a time series problem, is to stay simple. Most of the time the simpler solutions are the best ones in terms of accuracy and adaptability. They are also easier to maintain or embed and more persistent to possible data shifts. In this sense, the gold standard for time series modeling consists in the adoption of linear-based algorithms. They require few assumptions and simple data manipulations to produce satisfactory results.",
            "In this post, we carry out a sales forecasting task. We provide future forecasts using the standard linear regression and an improved version of it. We are referring to linear trees. They belong to the family of model trees, like classical decision trees, but are different because they compute linear approximations (instead of constant ones) fitting simple linear models in the leaves. The training is computed evaluating the best partitions on the data fitting multiple linear models. The final model is a tree-based structure with linear models in the leaves.",
            "A pythonic implementation of linear trees is available in linear-tree: a python library to build Model Trees with Linear Models at the leaves. The package is fully integrable with sklearn. It provides simple BaseEstimators that wrap every linear model present in sklearn.linear_model to build an optimal linear tree.",
            "For our experiment, we simulate some data that replicate store sales. We generate artificial sales history of 400 stores. Where store sales are influenced by many factors, including promotions, holidays, and seasonality. Our duty is to forecast the future daily sales for up to one year in advance.",
            "We don\u2019t use any past information when we provide our predictions. We engineer all our regressors to be accurate and accessible in any future time period. This enables us to provide long-time forecasts.",
            "We have 400 stores with historical sales from different years with visible daily seasonality.",
            "We aim to predict sales up to a year ahead for all the stores at our disposal. To make this possible, we build a different model at the store level. We operate parameters tuning on the training set for each store independently. We end with 400 models trained ad-hoc. The procedure is computed fitting simple linear regressions and linear trees. In this way, we can verify the goodness of 400 independent fits by comparing the performances on the unseen test data for both model types.",
            "Training a linear tree is simple as training a standard linear regression. When we train a linear tree, we are simply fitting multiple linear models on different data partitions. We can clearly understand that we benefit from the same advantages, i.e. simple preprocessing and good explicative power.",
            "Inspecting the tree paths of the various fits, we can understand the decision taken by the algorithm. In the case of linear trees, we identify which features are responsible to split the original dataset and provide the benefit of further fits.",
            "As we can see, different models are used for different months, weeks of the year, or also days of the week. This is not so surprising and is because a single linear regression can\u2019t generalize well on the whole dataset provided. Instead of manually searching for the perfect data partitions, linear trees evaluate and choose the best ones looking directly at the data.",
            "Predictions on test data from linear regressions look more stable around the means. Predictions made with linear trees suit well the data adapting better to the various seasonal patterns and reproducing the peeks. In the end, we compare the two model types in terms of performance. On the test data, we compute the Root Mean Squared Error (RMSE) for all the shops at our disposal. This enables us to verify how many times linear trees are better than linear regressions.",
            "Linear trees seem to outperform classical linear regressions more than 9 times on 10. This is a great result for us, which means that there is an advantage in using linear trees in our scenario.",
            "In this post, we carried out a time series forecasting task using linear models. We tested simple linear regressions and linear trees to predict synthetic future store sales. The results were interesting. Linear trees learned better representations of the data providing more accurate predictions most of the time. This is achieved simply by fitting multiple linear regressions in specified data partitions obtained following simple decision rules.",
            "CHECK MY GITHUB REPO",
            "Keep in touch: Linkedin"
        ]
    },
    "https://towardsdatascience.com/nerf-and-what-happens-when-graphics-becomes-differentiable-88a617561b5d": {
        "author": "Jonathan Laserson, PhD",
        "length": "8 min read",
        "title": "NeRF and What Happens When Graphics Becomes Differentiable",
        "description": "How the romance between deep learning and computer graphics began, and the future road towards photorealism",
        "tags": [
            "deep-learning",
            "nerf",
            "computer-graphics",
            "editors-pick",
            "thoughts-and-theory"
        ],
        "content": [
            "Rendering, a crucial part of any graphics system, is what makes a computerized three-dimensional world reflect on our two-dimensional computer screen, as if one of the world\u2019s characters had taken a camera out of their pocket and photographed what they saw.",
            "Over the past year (2020), we\u2019ve learned how to make the rendering process differentiable, and turn it into a deep learning module. This sparks the imagination, because the deep learning motto is: \u201cIf it\u2019s differentiable, we can learn through it\u201d. Indeed, many deep-learning breakthroughs have come from trying to soften the gap between zero and one, turning the discrete into the continuous, the argmax into the softmax. If we know how to differentially go from 3D to 2D, it means we can use deep learning and backpropagation to go back from 2D to 3D as well.",
            "Suppose I\u2019m holding in my hand a (2D) photograph of a cat sitting inside a window (taken in the real world), and have access to a differentiable renderer, a system that converts a representation of a three-dimensional (computerized) world to a two-dimensional image. Right now, if I ask the system to render a 2D image, I would get some random image that looks nothing like a cat, because the 3d world it would have rendered was just some randomly initialized computerized environment.",
            "BUT because the renderer is differentiable, it means that every single pixel in the resulting image is a differentiable function of the computerized 3d world. This essentially means that our deep learning framework maintains a link from the rendered image back to the 3D world representation. We can therefore ask: \u201chow can we change our current 3D world representation, so that the next time we render it from the same angle, the resulting image would look more like the photograph?\u201d (mathematically, we are taking the gradient of the difference between the photograph pixels and the image pixels).",
            "If we change our 3D world representation accordingly, and do this over and over, eventually our differentiable renderer will render an image that looks very similar to the original cat photograph.",
            "At that point, we can turn our attention to the computerized 3D world it converged into. What can we make of it? Does it have a 3D cat object sitting inside a 3D window? If we render the same 3D computerized world from a different camera angle (obtaining a different image), would it still look like a cat? Well, unless our 3D representation already has some predefined notion of what a 3D cat looks like, probably not. Using just a single photograph, from a single view point, there is just not enough information to understand how the cat will look like from a different angle. For all we know our 3D world might converge into a flat cat printed on a canvas that is placed just in front of the camera.",
            "In order for our representation of the 3d world to converge to something useful, we\u2019ll need more photographs, taken from more points of view.",
            "That\u2019s precisely what a group of researchers from UC Berkeley (from Ren Ng\u2019s lab) did in a project called NeRF (short for Neural Radiance Fields). They shot a static scene in the real world at no fewer than 40 different angles at once (and obtained 40 images), with the goal to get a three-dimensional computerized representation of the scene, that can be correctly rendered later from any angle. Their results are stunning.",
            "The idea of using multiple views of the same object or scene in order to get a 3D representation of it is not new (it\u2019s called view synthesis). However, none of the previous methods were able to generate a 3D structure with such a high fidelity to the real world that images rendered from it from new viewpoints were indistinguishable from real photos.",
            "The guys who built NeRF were able to nail it. And the reason they succeeded was that they combined between a powerful differentiable renderer and a powerful yet untypical 3D world representation. That powerful 3D world representation was a neural network. And not even one of those fancy neural networks, but rather a multi-layer perceptron just like the good old times, 9 fully-connected layers all the way through.",
            "How does the neural network at the center of NeRF represent the entire 3D world? The answer: It classifies its 3D space! Its input is any 3 coordinates (x, y, z) of a point in space (as well as the camera view point), and its output is the color (r, g, b) and the degree of opacity (\u03b1) of the material in that point in space.",
            "This is it. Five numbers in, four numbers out. Once the neural network is fully trained on the scene, we can query it on any point in space and probe the environment. This is nice, but if we want to know more about the actual content of the scene, we\u2019ll have to work hard probing the network. This neural network does not explicitly tell us anything about the number of objects in the scene, their class, or their boundaries. It\u2019s a much more \u201catomic\u201d, low-level, way to look upon the world.",
            "From a computer graphics researcher point of view, this is radical. They are used to represent each object in the scene in its own data-structure (called a \u201cmesh\u201d). They are used to process surfaces, obtain depth maps, and trace rays from light sources. There is none of that here.",
            "Instead, we have something more similar to the voxel grid used to represent volume in medical images, like MRI or CT scans, except:",
            "The MRI/CT volume has a fixed resolution, using discreet voxels, while we can ask the NeRF network about any fractional (x, y, z) point, at any resolution. The MRI/CT volume takes up a huge amount of space (consider 512 x 512 x 512 voxels) compared to the relatively compact 256 x 256 x 9 weights (roughly) of the NeRF multi-layer perceptron.",
            "So the NeRF\u2019s neural network is not the most natural way to represent distinct objects, but it is definitely a suitable way to render them, and perhaps this is what makes the resulting images so photorealistic.",
            "How does the NeRF renderer work?",
            "You can imagine the image we\u2019re rendering like a screen placed in front of the camera, perpendicular to the camera\u2019s view angle. To determine any pixel\u2019s color in this image, we send a ray from the camera through the pixel\u2019s position on the screen and onwards. If the ray hits any surface, the pixel\u2019s color is determined to be the color of the surface at the point of impact. If the surface is opaque (\ud835\udec2=1), we\u2019re done here. But if not, we have to keep following the ray to see what other surfaces it hits because their color will also impact that pixel\u2019s color.",
            "In the non-differentiable version of this renderer, only the closest opaque surface determined the color of the pixel. But if we want to make it differentiable, we need again to turn the argmax into a softmax. In other words, all the surfaces hit by the ray need to play some part in the computation, even those that are occluded by the closest one (which will still have the largest impact). This is key to maintaining that necessary link between all the regions of our 3D world and the resulting image pixels.",
            "More concretely, to calculate a particular pixel\u2019s color, we sample 100 points along the above mentioned ray, and ask our neural network for the color (r,g,b) and the degree of opacity (\ud835\udec2) of each. The pixel\u2019s final color is a linear combination (weighted average) of all the colors of all the sampled points, even those that are in mid-air (that should have \ud835\udec2 close to 0), or those on surfaces that are \u201chidden\u201d from the camera (that should have \ud835\udec2 close to 1). The weight assigned to the i-th point on the ray (away from the camera) will get the weight T_i \ud835\udec2_i, where the scalar \ud835\udec2_i",
            "is calculated directly by the network, and",
            "depends on the opacity values of the previous points. You can see how the closest surface gets the most dominant weight (we normalize the weights so they sum to 1).",
            "The Way Forward",
            "I find this very exciting. This is no longer a neural network that is predicting physics. This is physics (or optics) plugged on top of a neural network inside a PyTorch engine. We have now a differentiable simulation of the real world (harnessing the power of computer graphics) on top of a neural representation of it (harnessing the power of deep learning). There\u2019s no wonder that the results look so photorealistic.",
            "This romance between deep learning and graphics is only in its beginning. It is going to close the gap between computer simulation and the real world. If you have a VR headset, think about watching movies where you don\u2019t just sit in a static view point, but can actually move inside the scene, and even interact with it.",
            "The latest innovations will allow us to import objects and environments from the real world into the virtual world, relying less on artist-made assets. [In this context, I recommend watching Sanja Fidler of Nvidia\u2019s lecture on creating three-dimensional content from a workshop on the subject from the last NeurIPS.]",
            "And those things I mentioned earlier about the limitations of the NeRF network? That it is too low level for computer graphics? This is going to change. We\u2019re going to extract from this 3D world representation semantic elements that will allow us to understand it and control it, just like we do for the classic mesh representation, but retaining that new standard of photorealism.",
            "In the next post I will explain how we plan to do that.",
            "Further Reading",
            "[1] Mildenhall et. al. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. ECCV 2020 [link]",
            "[2] Facebook\u2019s PyTorch3D library. [link] [Tutorial on implicit functions and NeRF] [dedicated jupyter notebook for NeRF]",
            "[3] Sanja Fidler. A. I. For 3D Content Creation. NeurIPS 2020 [link]",
            "[4] Vladlen Koltun: Towards Photorealism [video]"
        ]
    },
    "https://towardsdatascience.com/xgboost-regression-explain-it-to-me-like-im-10-2cf324b0bbdb": {
        "author": "Shreya Rao",
        "length": "7 min read",
        "title": "XGBoost Regression: Explain It To Me Like I\u2019m 10",
        "description": "Getting Started",
        "tags": [
            "machine-learning",
            "algorithms",
            "xgboost",
            "data-science",
            "getting-started"
        ],
        "content": [
            "When I was just starting on my quest to understand Machine Learning algorithms, I would get overwhelmed with all the math-y stuff. I found it difficult to understand the math behind an algorithm without fully grasping the intuition. So I would gravitate towards sources that completely broke down the algorithm into simple steps and made it digestible to someone who never even heard the word Algorithm before. Okay, that is a blatant exaggeration, but you know what I mean. So that\u2019s what I\u2019m attempting to do now. Explaining the intuition behind the XGBoost Algorithm to a 10-year-old. Here goes!",
            "Let\u2019s start with our training dataset that consists of five people. We recorded their ages, whether or not they have a master\u2019s degree, and their salary (in thousands). Our goal is to predict Salary using the XGBoost Algorithm.",
            "Step 1: Make an Initial Prediction and Calculate Residuals",
            "This prediction can be anything. But let\u2019s assume our initial prediction is the average value of the variables we want to predict.",
            "We can calculate residuals using the following formula:",
            "Here, our Observed Values are the values in the Salary column and all Predicted Values are equal to 70 because that is what we chose our initial prediction to be.",
            "Step 2: Build an XGBoost Tree",
            "Each tree starts with a single leaf and all the residuals go into that leaf.",
            "Now we need to calculate something called a Similarity Score of this leaf.",
            "\u03bb (lambda) is a regularization parameter that reduces the prediction\u2019s sensitivity to individual observations and prevents the overfitting of data (this is when a model fits exactly against the training dataset). The default value of \u03bb is 1 so we will let \u03bb = 1 in this example.",
            "Now we should see if we can do a better job clustering the residuals if we split them into two groups using thresholds based on our predictors \u2014 Age and Master\u2019s Degree?. Splitting the Residuals basically means that we are adding branches to our tree.",
            "First, let\u2019s try splitting the leaf using Master\u2019s Degree?",
            "And then calculate the Similarity Scores for the left and right leaves of the above split:",
            "Now we need to quantify how much better the leaves cluster similar Residuals than the root does. We can do this by calculating the Gain of splitting the Residuals into two groups. If the Gain is positive, then it\u2019s a good idea to split, otherwise, it is not.",
            "Then I need to compare this Gain to those of the splits in Age. Since Age is a continuous variable, the process to find the different splits is a little more involved. First, need to arrange the rows of our dataset according to the ascending order of Age. Then we calculate the average values of the adjacent values in Age.",
            "Now we split the Residuals using the four averages as thresholds and calculate Gain for each of the splits. The first split uses Age < 23.5:",
            "For this split, we find the Similarity Score and Gain the same way we did for Master\u2019s Degree?",
            "Do the same thing for the rest of the Age splits:",
            "Out of the one Mater\u2019s Degree? split and four Age splits, the Master\u2019s Degree split has the greatest Gain value, so we\u2019ll use that as our initial split. Now we can add more branches to the tree by splitting our Master\u2019s Degree? leaves again using the same process described above. But, only this time, we use the initial Master\u2019s Degree? leaves as our root nodes and try splitting them by getting the greatest Gain value that is greater than 0.",
            "Let\u2019s start with the left node. For this node, we only consider the observations that have the value \u2018Yes\u2019 in Master\u2019s Degree? because only those observations land in the left node.",
            "So we calculate the Gain of the Age splits using the same process as before, but this time using the Residuals in the highlighted rows only.",
            "Since only Age < 25 gives us a positive Gain, we split the left node using this threshold. Moving onto our right node, we only look at values with \u2018No\u2019 values in Master\u2019s Degree?",
            "We only have two observations in our right node, so the only split possible is Age < 24.5 because that is the average of the two Age values in the highlighted rows.",
            "The Gain of this split is positive, so our final tree is:",
            "Step 3: Prune the Tree",
            "Pruning is another way we can avoid overfitting the data. To do this we start from the bottom of our tree and work our way up to see if a split is valid or not. To establish validity, we use \u03b3 (gamma). If Gain \u2014 \u03b3 is positive then we keep the split, otherwise, we remove it. The default value of \u03b3 is 0, but for illustrative purposes, let\u2019s set our \u03b3 to 50. From previous calculations we know the Gain values:",
            "Since Gain \u2014 \u03b3 is positive for all splits except that of Age < 24.5, we can remove that branch. So the resulting tree is:",
            "Step 4: Calculate the Output Values of Leaves",
            "We are almost there! All we have to do now is calculate a single value in our leaf nodes because we can not have a leaf node giving us multiple outputs.",
            "This is similar to the formula to calculate Similarity Score except we are not squaring the Residuals. Using the formula and \u03bb = 1, *drum roll* our final tree is:",
            "Step 5: Make New Predictions",
            "Now that all that hard model building is behind us, we come to the exciting part and see how much our predictions improve using our new model. We can make predictions using this formula:",
            "The XGBoost Learning Rate is \u025b (eta) and the default value is 0.3. So the predicted value of our first observation will be:",
            "Similarly, we can calculate the rest of the predicted values:",
            "Step 6: Calculate Residuals Using the New Predictions",
            "We see that the new Residuals are smaller than the ones before, this indicates that we\u2019ve taken a small step in the right direction. As we repeat this process, our Residuals will get smaller and smaller indicating that our predicted values are getting closer to the observed values.",
            "Step 7: Repeat Steps 2\u20136",
            "Now we just repeat the same process over and over again, building a new tree, making predictions, and calculating Residuals at each iteration. We do this until the Residuals are super small or we reached the maximum number of iterations we set for our algorithm. If the tree we built at each iteration is indicated by T\u1d62, where i is the current iteration, then the formula to calculate predictions is:",
            "And that\u2019s it. Thanks for reading and good luck with the rest of your algorithmic journey!",
            "My learning was greatly enhanced by Josh Starmer from StatQuest and this article was hugely inspired by his videos on XGBoost. For more on Machine Learning and Statistics, check out StatQuest!"
        ]
    },
    "https://medium.datadriveninvestor.com/4-simple-steps-in-building-ocr-1f41c66099c1": {
        "author": "Naga Kiran",
        "length": "4 min read",
        "title": "4 Simple steps in building OCR",
        "description": "DDI Editor's Pick: 5 Machine Learning Books That Turn You from Novice to Expert - Data Driven\u2026",
        "tags": [
            "machine-learning",
            "ocr",
            "python",
            "nlp",
            "image"
        ],
        "content": [
            "Optical character recognition (OCR) is process of classifying optical patterns contained in a digital image. The character recognition is achieved through segmentation, feature extraction and classification.",
            "OCR (optical character recognition) is the recognition of printed or written text characters by a computer. This involves photoscanning of the text character-by-character, analysis of the scanned-in image, and then translation of the character image into character codes, such as ASCII, commonly used in data processing.",
            "In OCR processing, the scanned-in image or bitmap is analyzed for light and dark areas in order to identify each alphabetic letter or numeric digit. When a character is recognized, it is converted into an ASCII code. Special circuit boards and computer chips designed expressly for OCR are used to speed up the recognition process.",
            "www.datadriveninvestor.com",
            "Steps in Optical Character Recognition :-",
            "1) Extraction of Character boundaries from Image,",
            "2) Building a Convolutional Neural Network(ConvNet) in remembering the Character images,",
            "3) Loading trained Convolutional Neural Network(ConvNet) Model,",
            "4) Consolidating ConvNet predictions of characters",
            "github.com",
            "The Algorithm is built in a way to segment each individual character in a Image as individual images :-) , followed by recognition and consolidation to text in an Image.",
            "to download the Pretrained Models . to download sample labelled character Images train data.",
            "1) Optical Scanning \u2702\ufe0f from Image :",
            "Select any document or letter of having text information",
            "Extract Character boundaries: Contours can be explained simply as a curve joining all the continuous points (along the boundary). The contours are a useful tool for shape analysis and object detection and recognition. Here Contours explained in differentiating each individual character in an image with using contour dilation technique. Create a boundary to each character in an image with using OpenCV Contours method. Character recognition with the use ofOpenCV contours method. OpenCV code implementation in differentiating the words with the use of contours",
            "ret,thresh1 = cv2.threshold(im1,180,255,cv2.THRESH_BINARY_INV) kernel = np.ones((5,5),np.uint8) dilated = cv2.dilate(thresh1,kernel,iterations = 2) _,contours, hierarchy = cv2.findContours(dilated,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) cordinates = [] for cnt in contours: x,y,w,h = cv2.boundingRect(cnt) cordinates.append((x,y,w,h)) #bound the images cv2.rectangle(im,(x,y),(x+w,y+h),(0,255,0),1) cv2.namedWindow('BindingBox', cv2.WINDOW_NORMAL) cv2.imwrite('data/BindingBox4.jpg',im)",
            "Naming Convention followed(Labelling) : The extracted text characters should be labelled with the Original character name associated with it.",
            "Naming convention followed here is, last letter of file name should be the name associated with the character for pre-processing the images data.",
            "Pre-processing",
            "The raw data depending on the data acquisition type is subjected to a number of preliminary processing steps to make it usable in the descriptive stages of character analysis. The image resulting from scanning process may contain certain amount of noise Smoothing implies both filling and thinning. Filling eliminates small breaks, gaps and holes in digitized characters while thinning reduces width of line.",
            "(a) noise reduction (b) normalization of the data and (c) compression in the amount of information to be retained.",
            "2) Build a ConvNet Model \u2702\ufe0f(Character Recognition Model):",
            "Convolution Network of 8 layers with 2*4 layers residual feedbacks used in remembering the Patterns \u2702\ufe0f of the Individual Character Images.",
            "1st Model will train on the Individual Character Images with direct Classification to predict the Images with softmax Classification of Character Categories. 2nd Model is same model with last before layer as predictor which will Calculate a Embedding of specified Flatten Neurons ( The Predicted flatten Values will have Feature Information of Receipt Images ).",
            "3) Load Trained ConvNet OCR model:",
            "Optical Character recognition last step involves preprocessing of image into specific word related contours and letter contours, followed by prediction and consolidating according to letter and word related contours in an image.",
            "once after training the model, we can save and load the pre-trained Optical character recognition model.",
            "4) Test and Consolidate Predictions of OCR :",
            "Consolidate predictions involves, assigning specific ID to each word related contour with the line associated with the word in image, Consolidating all predictions in a sorted series of specific word related contour and letters associated word."
        ]
    },
    "https://medium.com/neo4j/article-recommendation-with-personalized-pagerank-and-full-text-search-c0203dd833e8": {
        "author": "Mark Needham",
        "length": "4 min read",
        "title": "Article recommendation with Personalized PageRank and Full Text Search",
        "description": "",
        "tags": [
            "neo4j",
            "pagerank",
            "recommendations",
            "graph-algorithms",
            "data-science"
        ],
        "content": [
            "6 months ago Tomaz Bratanic wrote a great blog post showing how to build an article recommendation engine using NLP techniques and the Personalized PageRank algorithm from the Graph Algorithms library.",
            "Update: The O\u2019Reilly book \u201cGraph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.com",
            "In the post Tomaz extracts key words for each article using the GraphAware NLP library, and then runs PageRank in the context of articles based on these key words.",
            "I was curious whether I could create a poor man\u2019s version of Tomaz\u2019s work using the Full Text Search functionality that was added in Neo4j 3.5, and so here we are!",
            "Tomaz explains how to import the data in his post, so we\u2019ll continue from there. The diagram below shows the graph model that we\u2019ll be working with. We have articles written by authors, and those articles can reference each other.",
            "The first thing we need to do is create a Full Text Search index for our Article nodes. We\u2019ll index the title and abstract properties on these nodes.",
            "CALL db.index.fulltext.createNodeIndex('articlesAll', ['Article'], ['title', 'abstract'])",
            "We can check on the progress of the index creation by running the following query:",
            "CALL db.indexes()",
            "It will have a state of POPULATING while node properties are being added to the index. This state will change to ONLINE once it\u2019s done. The following query will block until the index is online:",
            "CALL db.index.fulltext.awaitIndex(\"articlesAll\")",
            "Now that we\u2019ve done this, let\u2019s get on with the algorithms.",
            "Tomaz first explores articles that contain the phrase \u201csocial networks\u201d. Let\u2019s create a parameter containing that search term:",
            ":param searchTerm => '\"social networks\"'",
            "Not that we\u2019ve put the search term in quotes. We do this so that Full Text Search will treat the term as a phrase rather than interpreting each term separately.",
            "Now we want to call the PageRank algorithm from the point of view of articles that contain this search term. Let\u2019s first see how many articles the full text index comes back with:",
            "CALL db.index.fulltext.queryNodes(\"articlesAll\", $searchTerm) YIELD node, score RETURN count(*)",
            "Just under 15,000 nodes, or around 0.5% of all articles are returned by the query. The following query will return the top 10 articles for the search term:",
            "CALL db.index.fulltext.queryNodes(\"articlesAll\", $searchTerm) YIELD node, score RETURN node.id, node.title,  score LIMIT 10",
            "Now we can feed these nodes into the PageRank algorithm as the sourceNodes config parameter. This will bias the results of the algorithm around these nodes.",
            "The following query will find us the most influential articles about social networks:",
            "CALL db.index.fulltext.queryNodes(\"articlesAll\", $searchTerm) YIELD node WITH collect(node) as articles CALL algo.pageRank.stream('Article', 'REFERENCES', { sourceNodes: articles }) YIELD nodeId, score WITH nodeId,score ORDER BY score DESC LIMIT 10 RETURN algo.getNodeById(nodeId).title as article, score",
            "As in Tomaz\u2019s post, Sergey Brin and Larry Page\u2019s paper describing Google shows up in first place.",
            "In the next part of the post, Tomaz shows how we can write queries to find papers that would be interesting to researchers in different fields.",
            "Recommendation of articles described by keyword \u201centropy\u201d from the point of view of Jose C. Principe.",
            "Let\u2019s setup parameters:",
            ":param authorName => \"Jose C. Principe\"; :param searchTerm => \"entropy\"",
            "And now run the query:",
            "MATCH (a:Article)-[:AUTHOR]->(author:Author) WHERE author.name=$authorName WITH author, collect(a) as articles CALL algo.pageRank.stream( 'CALL db.index.fulltext.queryNodes(\"articlesAll\", $searchTerm) YIELD node RETURN id(node) as id', 'MATCH (a1:Article)-[:REFERENCES]->(a2:Article) RETURN id(a1) as source,id(a2) as target', { sourceNodes: articles, graph:'cypher', params: {searchTerm: $searchTerm}}) YIELD nodeId, score WITH author, nodeId, score WITH algo.getNodeById(nodeId) AS n, score WHERE not(exists((author)-[:AUTHOR]->(n))) RETURN n.title as article, score, [(n)-[:AUTHOR]->(author) | author.name][..5] AS authors order by score desc limit 10",
            "We\u2019ll see these results:",
            "And what about if we run the same query for a different author?",
            ":param authorName => \"Hong Wang\";",
            "We\u2019ll see this results:",
            "We don\u2019t get exactly the same results as Tomaz, but we do still get a different set of results for the different authors.",
            "So in summary, it does seem that we can get a reasonable approximation of Tomaz\u2019s post using Neo4j\u2019s Full Text Search functionality.",
            "If you have any other ideas of what we can do with this dataset, let me know by emailing devrel@neo4j.com"
        ]
    },
    "https://medium.com/swlh/create-rest-api-with-django-and-neo4j-database-using-django-nemodel-1290da717df9": {
        "author": "SIHEM BOUHENNICHE",
        "length": "8 min read",
        "title": "Create REST API With Django and Neo4j Database Using Django_nemodel",
        "description": "",
        "tags": [
            "neo4j",
            "django",
            "rest-api",
            "graph-database",
            "python"
        ],
        "content": [
            "In this tutorial, I\u2019ll show you how to create a REST API using Django and Neo4j database through a simple example. In order to create this API, we need to use an Object Graph Mapper (OGM) to request the graph database, that\u2019s why we will use django_neomodel which is a Django integration of the awesome OGM neomodel.",
            "Django : Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Neo4j : Neo4j is a NoSQL graph database management system (DBMS). Django_neomodel : a module that allows you to use the n eo4j graph database with Django using neomodel .",
            "Installing dependencies. Create and set up the project. Create the graph structure that Django_neomodel ORM will manage. Create and manage some objects using Django shell. Create views and urls patterns. Request our API.",
            "Before jumping into the installation, you should have already downloaded python. You can check the actual version for python by this command:",
            "$ python --version",
            "The first step is to create a virtual environment that will will englobe all the requirements. So let\u2019s start by installing virtualenv and activate the virtual environment:",
            "$ pip install virtualenv $ virtualenv env $ env\\Scripts\\activate",
            "Then we install django and django_neomodel:",
            "$ pip install Django $ pip install django_neomodel",
            "Finally, we install Neo4j. For that i recommend you to use the official documentation of Neo4j. You can find it here.",
            "We are ready now to start creating our django API.",
            "First of all, we start by creating a neo4j project named \u201cTutorial\u201d. Then we create a graph database inside it. (look at the figure below)",
            "Now, it\u2019s time to create django project. For that, we use this command:",
            "$ django-admin startproject myproject",
            "Then, according to the django project structure we must create an application inside this project:",
            "$ cd myproject $ python manage.py startapp myapi",
            "After that, we need to register our application in myproject/settings.py file so Django can recognize this new application. We must also register django_neomodel and set up the connexion to our neo4j database.",
            "# Application definition INSTALLED_APPS = [ # django.contrib.auth etc 'myapi.apps.MyapiConfig', 'django_neomodel' ] # Database # https://docs.djangoproject.com/en/3.0/ref/settings/#databases NEOMODEL_NEO4J_BOLT_URL = os.environ.get('NEO4J_BOLT_URL','bolt://username:password@localhost:7687') # you are free to add this configurations NEOMODEL_SIGNALS = True NEOMODEL_FORCE_TIMEZONE = False NEOMODEL_ENCRYPTED_CONNECTION = True NEOMODEL_MAX_POOL_SIZE = 50",
            "Our example is very simple. We will have two entities Person and City and two relationships between them(LivesIn and Friend). see the structure below:",
            "Let\u2019s define our graph structure inside myapi/models.py file like that :",
            "from neomodel import StructuredNode, StringProperty, IntegerProperty,UniqueIdProperty, RelationshipTo # Create your models here. class City(StructuredNode): code = StringProperty(unique_index=True, required=True) name = StringProperty(index=True, default=\"city\") class Person(StructuredNode): uid = UniqueIdProperty() name = StringProperty(unique_index=True) age = IntegerProperty(index=True, default=0) # Relations : city = RelationshipTo(City, 'LIVES_IN') friends = RelationshipTo('Person','FRIEND')",
            "You can find more details about how to define node entities and relationships in the neomodel official documentation.",
            "After that, we must create constraints and indexes for our labels to ensure that everything is right by typing this command :",
            "$ python manage.py install_labels",
            "You can ensure that constraints and were applied in the Neo4j browser like this :",
            "Now, let\u2019s ensure that everything is all right by populating our database by some persons and cities. For that, we will use simply Django shell:",
            "$ python manage.py shell Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. (InteractiveConsole) >> from myapi.models import * >> all_persons = Person.nodes.all() [] >> all_cities = City.nodes.all() [] >> algiers = City(code=\"ALG\",name=\"Algiers\") >> print(algiers) {code: \"ALG\", name: \"Algiers\"} >> algiers.save() <City: {code: \"ALG\", name: \"Algiers\", id: 0}> >> sihem = Person(name='Sihem', age=24) >> print(sihem) {'uid': '56329dc', 'name': 'Sihem', 'age': 24} >> sihem.save() <Person: {'uid': '56329dc', 'name': 'Sihem', 'age': 24, 'id': 1}> >> sihem.city.connect(algiers) True >> if sihem.city.is_connected(algiers): ...     print(\"sihem lives in Algeirs\") ... sihem lives in Algeirs >> oussama= Person(name='Oussama', age=22) >> oussama.save() <Person: {'uid': 'c139f04', 'name': 'Oussama', 'age': 22, 'id': 21}> >> oussama.city.connect(algiers) True >> oussama.friends.connect(sihem) True",
            "After that, you can check our objects in Neo4j database by using Cypher queries.",
            "You can create more objects using either Neo4j browser or Django shell.",
            "Arriving at this stage, it\u2019s time to create our person and city views. So let\u2019s delete myapi/views file and use instead of it a new package named myapi/views. We wil have the following application structure:",
            "myapi |_migrations |_models | |_ __init__.py | |_person.py | |_city.py |_views | |_ __init__.py | |_ person.py | |_ city.py | |_ connectors.py |_ ......",
            "Now, lets define the content of each file of mayapi/views package :",
            "person.py : this file contains the following methods :",
            "personDetails : in which we will put our CRUD operations (create new person, update, delete and get details of a person). getAllPersons : which will render a list of existing Person instances.",
            "from django.http import JsonResponse from myapi.models import Person from django.views.decorators.csrf import csrf_exempt import json def getAllPersons(request): if request.method == 'GET': try: persons = Person.nodes.all() response = [] for person in persons : obj = { \"uid\": person.uid, \"name\": person.name, \"age\": person.age, } response.append(obj) return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) @csrf_exempt def personDetails(request): if request.method == 'GET': # get one person by name name = request.GET.get('name', ' ') try: person = Person.nodes.get(name=name) response = { \"uid\": person.uid, \"name\": person.name, \"age\": person.age, } return JsonResponse(response, safe=False) except : response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'POST': # create one person json_data = json.loads(request.body) name = json_data['name'] age = int(json_data['age']) try: person = Person(name=name, age=age) person.save() response = { \"uid\": person.uid, } return JsonResponse(response) except : response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'PUT': # update one person json_data = json.loads(request.body) name = json_data['name'] age = int(json_data['age']) uid = json_data['uid'] try: person = Person.nodes.get(uid=uid) person.name = name person.age = age person.save() response = { \"uid\": person.uid, \"name\": person.name, \"age\": person.age, } return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'DELETE': # delete one person json_data = json.loads(request.body) uid = json_data['uid'] try: person = Person.nodes.get(uid=uid) person.delete() response = {\"success\": \"Person deleted\"} return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False)",
            "city.py : this file contains the following methods :",
            "cityDetails : in which we will put our CRUD operations (create new city, update, delete and get details of a city). getAllCities : which will render a list of existing City instances.",
            "from django.http import JsonResponse from myapi.models import City from django.views.decorators.csrf import csrf_exempt import json def getAllCities(request): if request.method == 'GET': try: cities = City.nodes.all() response = [] for city in cities: obj = { \"code\": city.code, \"name\": city.name, } response.append(obj) return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) @csrf_exempt def cityDetails(request): if request.method == 'GET': # get one city by name name = request.GET.get('name', ' ') try: city = City.nodes.get(name=name) response = { \"code\": city.code, \"name\": city.name, } return JsonResponse(response, safe=False) except : response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'POST': # create one city json_data = json.loads(request.body) name = json_data['name'] code = json_data['code'] try: city = City(name=name, code=code) city.save() response = { \"code\": city.code, } return JsonResponse(response) except : response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'PUT': # update one city json_data = json.loads(request.body) name = json_data['name'] code = json_data['code'] try: city = City.nodes.get(code=code) city.name = name city.save() response = { \"code\": city.code, \"name\": city.name, } return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) if request.method == 'DELETE': # delete one city json_data = json.loads(request.body) code = json_data['code'] try: city = City.nodes.get(code=code) city.delete() response = {\"success\": \"City deleted\"} return JsonResponse(response) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False)",
            "connectors.py : this file contains the following methods :",
            "connectPaC : which will create a connexion between a person and a city. connectPaP: which will create a connexion between two persons.",
            "from django.http import JsonResponse from myapi.models import * from django.views.decorators.csrf import csrf_exempt import json @csrf_exempt def connectPaC(request): if request.method == 'PUT': json_data = json.loads(request.body) uid = json_data['uid'] code = json_data['code'] try: person = Person.nodes.get(uid=uid) city = City.nodes.get(code=code) res = person.city.connect(city) response = {\"result\": res} return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False) @csrf_exempt def connectPaP(request): if request.method == 'PUT': json_data = json.loads(request.body) uid1 = json_data['uid1'] uid2 = json_data['uid2'] try: person1 = Person.nodes.get(uid=uid1) person2 = Person.nodes.get(uid=uid2) res = person1.friends.connect(person2) response = {\"result\": res} return JsonResponse(response, safe=False) except: response = {\"error\": \"Error occurred\"} return JsonResponse(response, safe=False)",
            "__init__.py : this file makes Python treat myapi/views directory as one module.",
            "from myapi.views.person import * from myapi.views.city import * from myapi.views.connectors import *",
            "After that, we create new file inside myapi/ directory called urls in which we will register our list routes URLs to our views.",
            "from django.conf.urls import url from myapi.views import * from django.urls import path urlpatterns = [ path('person',personDetails), path('getAllPersons',getAllPersons), path('city',cityDetails), path('getAllCities',getAllCities), path('connectPaC',connectPaC), path('connectPaP',connectPaP) ]",
            "Then, we must register this list routes URLs inside myproject/urls.py file like that :",
            "from django.contrib import admin from django.urls import path from django.conf.urls import include urlpatterns = [ path('admin/', admin.site.urls), path('', include('myapi.urls')), ]",
            "Finally, we will request our API to ensure that everything works correctly. so let\u2019s start our API by runing this command :",
            "$ python manage.py runserver Watching for file changes with StatReloader Performing system checks... System check identified no issues (0 silenced). July 29, 2020 - 16:48:13 Django version 3.0.8, using settings 'myproject.settings' Starting development server at http://127.0.0.1:8000/",
            "To request the API, i used Postman which simplify making CRUD requests.",
            "At the end, we can illustrate the architeture of our API like that :",
            "Thank you for reading, if you have any questions or remarks please do not hesitate to leave a comment below",
            "Here is the repository of the whole project in this github link :",
            "github.com"
        ]
    },
    "https://thoughts.t37.net/how-we-reindexed-36-billions-documents-in-5-days-within-the-same-elasticsearch-cluster-cd9c054d1db8": {
        "author": "Fred de Villamil \ud83d\ude48 \ud83d\ude49 \ud83d\ude4a",
        "length": "12 min read",
        "title": "How we reindexed 36 billion documents in 5 days within the same Elasticsearch cluster",
        "tags": [
            "elasticsearch",
            "infrastructure",
            "linux"
        ],
        "content": [
            "Top highlight",
            "This article and much more is now part of my FREE EBOOK Running Elasticsearch for Fun and Profit available on Github. Fork it, star it, open issues and send PRs!",
            "At Synthesio, we use ElasticSearch at various places to run complex queries that fetch up to 50 million rich documents out of tens of billion in the blink of an eye. Elasticsearch makes it fast and easily scalable where running the same queries over multiple MySQL clusters would take minutes and crash a few servers on the way. Every day, we push Elasticsearch boundaries further, and going deeper and deeper in its internals leads to even more love.",
            "Last week, we decided to reindex a 136TB dataset with a brand new mapping. Updating an Elasticsearch mapping on a large index is easy until you need to change an existing field type or delete one. Such updates require a complete reindexing in a separate index created with the right mapping so there was no easy way out for us.",
            "We've called our biggest Elasticsearch cluster \"Blackhole\", because that's exactly what it is: a hot, ready to use datastore being able to contain virtually any amount of data. The only difference with a real blackhole is that we can get our data back at the speed of light.",
            "When we designed blackhole, we had to chose between 2 different models.",
            "A few huge machines with 4 * 12 core CPU, 512GB of memory and 36 800GB SSD drives, each of them running multiple instances of Elasticsearch. A lot of smaller machines we could scale horizontally as the cluster grows.",
            "We opted for the latter since it would make scaling much easier and didn't require spending too much money upfront.",
            "Blackhole runs on 75 physical machines:",
            "* 2 http nodes, one in each data center behind a HAProxy to load balance the queries.* 3 master nodes located in 3 different data center.* 70 data nodes into 2 different data center.",
            "Each node has quad core Xeon D-1521 CPU running at 2.40GHz and 64GB of memory. The data nodes have a RAID0 over 4*800GB SSD drives with XFS. The whole cluster runs a Systemd less Debian Jessie with a 3.14.32 vanilla kernel. The current version of the cluster has 218,75TB of storage and 4,68TB of memory with 2.39TB being allocated to Elasticsearch heap. That's all for the numbers.",
            "Blackhole runs ElasticSearch 1.7.5 on Java 1.8. Indexes have 12 shards and 1 replica. We ensure each data center hosts 100% of our data using Elasticsearch rack awareness feature. This setup allows to crash a whole data center without neither data loss nor downtime, which we test every month.",
            "All the filtered queries are ran with _cache=false. ElasticSearch caches the filtered queries result in memory, making the whole cluster explode at the first search. Running queries on 100GB shards, this is not something you want to see.",
            "When running in production, our configuration is:",
            "routing: allocation: node_initial_primaries_recoveries: 20 node_concurrent_recoveries: 20 cluster_concurrent_rebalance: 20 disk: threshold_enabled: true watermark: low: 60% high: 78% index: number_of_shards: 12 number_of_replicas: 1 merge: scheduler: max_thread_count: 8 type: 'concurrent' policy: type: 'tiered' max_merged_segment: 100gb segments_per_tier: 4 max_merge_at_once: 4 max_merge_at_once_explicit: 4 store: type: niofs query: bool: max_clause_count: 10000 action: auto_create_index: false indices: recovery: max_bytes_per_sec: 2048mb fielddata: breaker: limit: 80% cache: size: 25% expire: 1m store: throttle: type: 'none' discovery: zen: minimum_master_nodes: 2 ping: multicast: enabled: false unicast: hosts: [\"master01\",\"master02\",\"master03\"] threadpool: bulk: queue_size: 3000 type: cached index: queue_size: 3000 type: cached bootstrap: mlockall: true memory: index_buffer_size: 10% http: max_content_length: 1024mb",
            "After trying both ElasticSearch default_fs and mmapfs, we\u2019ve picked up niofs for file system storage.",
            "The NIO FS type stores the shard index on the file system (maps to Lucene NIOFSDirectory) using NIO. It allows multiple threads to read from the same file concurrently.",
            "The reason why we decided to go with niofs is to let the kernel manage the file system cache instead of relying on the broken, out of memory error generator mmapfs.",
            "We launch the java virtual machine with -Xms31g -Xmx31g. Combined with ElasticSearch mlockall=true, it ensures ElasticSearch gets enough memory to run and never swaps. The remaining 33GB are used for ElasticSearch threads and file system cache.",
            "Despite ElasticSearch recommendations we have replaced the Concurrent Mark Sweep (CMS) garbage collector with the Garbage First Garbage Collector (G1GC). With CMS, we would run into a stop the world garbage collection for every single query on more than 1 month of data.",
            "Our configuration of G1GC is relatively simple but does the job under pressure:",
            "JAVA_OPTS=\u201d$JAVA_OPTS -XX:-UseParNewGC\u201d JAVA_OPTS=\u201d$JAVA_OPTS -XX:-UseConcMarkSweepGC\u201d JAVA_OPTS=\u201d$JAVA_OPTS -XX:+UseCondCardMark\u201d JAVA_OPTS=\u201d$JAVA_OPTS -XX:MaxGCPauseMillis=200\" JAVA_OPTS=\u201d$JAVA_OPTS -XX:+UseG1GC \u201c JAVA_OPTS=\u201d$JAVA_OPTS -XX:GCPauseIntervalMillis=1000\" JAVA_OPTS=\u201d$JAVA_OPTS -XX:InitiatingHeapOccupancyPercent=35\"",
            "We started the initial indexing mid December 2015. It took 19 days from fetching the raw data to pushing it into ElasticSearch.",
            "Back then, Blackhole only had 46 nodes:",
            "3 master nodes 1 query node 42 data nodes",
            "This led to a cluster sized for 30 months of data with 1.29TB of memory and 134TB of storage, all SSD.",
            "For this initial indexing, we decided to go with 1 index per month and 30 shards per index. This didn't work as expected as each query on a month would request data from 3TB and 1.2 billion documents. As most queries went on 3 to 12 months, this made the cluster impossible to scale properly.",
            "The first part of the process took 10 days. We had to fetch 30 billion documents from our main Galera datastore, turn it into JSON and push it into a Kafka queue, each month of data being pushed into a different Kafka partition. Since we were scanning the database incrementally, the process went pretty fast considering the amount of data we were processing.",
            "The migration processes were running on 8 virtual machines with 4 core and 8GB RAM. Each machine was running a 8 processes of a Scala homemade program.",
            "During the second part, we merged the data from the Kafka with data from 2 other Galera clusters and an Elasticsearch cluster before pushing them into Blackhole.",
            "The merge and indexing parts took place on 8 virtual machines, each having 4 core and 8GB RAM. Each machine was running 8 indexing processes reading an offset of a Kafka partition.",
            "The indexer was shard aware. It had a mapping between the index it was writing on, its shards and the data node they were hosted on. This allowed to index directly on the right data nodes with the lowest possible network latency.",
            "This part was not as smooth as we expected.",
            "The first version of the indexer was developed in Scala, but for some reasons was slow as hell, not being able to index more than 30,000 documents per second. We rewrote it in Go in 2 days, and it was much better, with an average of 60,000 indexed documents per second, with peaks at 120,000 documents per second.",
            "Surprisingly, the main bottleneck was neither one of the Galera clusters nor the Elasticsearch metadata cluster, but the Kafka queues. For some reasons, we could not read more than 10,000 documents per second per Kafka partition.",
            "The other unexpected bottleneck was the CPU. Surprisingly, we were CPU bound but the disks were not a problem (which is normal since we're using SSDs).",
            "After 9 days, the data was fully indexed and we could start playing with the data.",
            "When we decided to change Blackhole mapping, we had enough experience with the cluster and its content to avoid previous mistakes and go much faster.",
            "Instead of monthly indexes, we decided to split the cluster into daily indexes. A few tests on a migrating index showed it was the way to go.",
            "With the new mapping dropping a bunch of data, we moved from 3GB for 1 million documents (with a replica) to 2GB for 1 million documents. Going daily reduced the average index from 3TB to 120GB, and a single shard from 100GB to 10GB. Having a large number of machines, this allowed to better use the ressources, starting with the JVM heap, running parallel queries.",
            "Instead of polling the data from our database clusters, we decided to reuse the data from Blackhole itself. This meant reading and writing on the same cluster simultaneously, adding some fun in the operation.",
            "This time, we did not use separate virtual machines to host the indexing processes. Instead, we decided to run the indexers on the data nodes, read locally and write on their counterpart in the secondary data center. Considering a 10Gb link and a 46ms network latency, that solution was acceptable. It meant we had 70 machines to both read and write to, allowing maximum parallelism.",
            "There are many solutions to copy an Elasticsearch index to another, but most of them neither allow splitting one to many or change the data model. Unfortunately, the new mapping involved deleting some fields and moving other fields somewhere else. Since we did not have the time to build a homemade solution, we decided to go with Logstash.",
            "Logstash has both an Elasticsearch input, for reading, an Elasticsearch output, for writing, and a transform filter to change the data model. The input module accepts a classic Elasticsearch query and the output module can be parallelized.",
            "We ran a few tests on Blackhole to determine which configuration was the best, and ended with 5000 documents scrolls and 10 indexing workers.",
            "For these tests, we were running with a production configuration, which explains the refreshes and segment count madness. Indeed, running with 0 replica was faster, but since we're using RAID0, this configuration was a no go.",
            "During the operation, both source and target nodes behaved without problems, specifically on the memory level.",
            "For the first tests, we ran logstash against a full day of reindexation, using a simple Elasticsearch query:",
            "query => '{ \"query\": { \"range\": { \"date\": { \"gte\": \"yyyy-mm-ddT00:00.000\", \"lte\": \"yyyy-mm-dd+1T00:00.000+01:00\" } } } }",
            "Unfortunately, for some reasons, we had missing documents because our scroll keepalive of 5 minutes was too short. This made catching up with the data too long as we had to replay the whole day, so we decided to run hourly queries.",
            "input { elasticsearch { hosts => [ \"local elasticsearch node\" ] index => \"index to read from\" size => 5000 scroll => \"20m\" # 5 minutes initial docinfo => true query => '{ \"query\": { \"range\": { \"date\": { \"gte\": \"2015-07-23T10:00.000+01:00\", \"lte\": \"2015-07-23T11:00.000+01:00\" } } } }' } } output { elasticsearch { host => \"remote elasticsearch node\" index => \"index to write to\" protocol => \"http\" index_type => \"%{[ @metadata ][_type]}\" document_id => \"%{[ @metadata ][_id]}\" workers => 10 } stdout { codec => rubydebug # because removing the timestamp field makes logstash crash } } filter { mutate { rename => { \"some field\" => \"some other field\" } rename => { \"another field\" => \"somewhere else\" } remove_field => [ \"something\", \"something else\", \"another field\", \"some field\", \" @timestamp \", \" @version \" ] } }",
            "We changed only a few settings for that reindexing.",
            "memory: index_buffer_size: 50% (instead of 10%) index: store: throttle: type : \"none\" (as fast as your SSD can go) translog: disable_flush: true refresh_interval: -1 (instead of 1s) indices: store: throttle: max_bytes_per_sec: \"2gb\"",
            "We wanted to limit the Lucene refreshes as much as we could, preferring to manage hundreds of thousand segments instead of limiting our throughput for CPU overhead.",
            "To manage the indexing process, we have created 2 simple tools: Yoko and Moulinette.",
            "Yoko and Moulinette use a simple MySQL database with every index to process, query to run and status. The data model is pretty self explanatory:",
            "CREATE TABLE `yoko` ( `index_from` varchar(16) NOT NULL, `index_to` varchar(16) NOT NULL, `logstash_query` text NOT NULL, `status` enum(\"todo\", \"processing\", \"done\", \"complete\", \"failed\") DEFAULT \"todo\" );",
            "Before indexing, we fill in the Yoko database with every index we want to migrate along with all the logstash queries we need to run. One line contains the source index, destination index and the query to reindex 1 hour of data.",
            "Yoko is a simple Python daemon that manages the global indexing processes. It:",
            "Creates the daily indexes when they don't exist yet with the right mapping. Checks for every \"done\" daily index and compares the number of documents from the initial index running the logstash query. Moves each successful \"done\" line to \"complete\" if the count matches or \"failed\". Delete each monthly index when every day of a month is \"complete\". Changes the refresh values when a daily index is \"complete\".",
            "PUT /index/_settings?master_timeout=120s { \"translog.disable_flush\" : \"false\", \"index\" : { \"refresh_interval\" : \"1s\" } }",
            "Moulinette is the processing script. It's a small daemon written in Bash (with some ugly bashisms) that runs on every indexing node. It fetches lines in \"todo\" from the yoko table, generates the logstash.conf with the source and destination index, and source and destination node and Logstash query. Then it runs Logstash, and once Logstash exits, switches the line to \"done\" if Logstash exit code is 0, or \"failed\" otherwise.",
            "Once again, the main problem was being CPU bound. As you can see on that Marvel screenshot, the cluster was put under heavy load during the whole indexing process. Considering that we were both reading and writing on the same cluster, with an indexing rate over 90,000 documents per second with 140,000 documents per second peaks, this is not surprising at all.",
            "Having a look at the CPU graphs, there was little we could to to improve the throughput without dropping Logstash and relying on a faster solution running on less nodes.",
            "The disks operations show well the scroll / index processing. There was certainly some latency inside Logstash for the transform process, but we didn't track it.",
            "The other problem was losing nodes. We had some hardware issues and lost some nodes here and there. This caused indexing from that node to crash and indexing to that node to stale since Logstash does not exit when the output endpoint crashes.",
            "This caused many lost time checking (almost) manually logs on every node once or twice a day. If an hourly index took more than 3 hours to process, we would consider it lost and restart Moulinette and move the hourly index to \"todo\".",
            "Lesson learned, Yoko and Moulinette V2 will have a better silent error handling. When an index is blocked for more than 3 hours, Yoko will raise an alert and move the index to \"todo\". The alert will allow to kill the locked Logstash process and restart Moulinette as soon as there's a problem.",
            "The next step is optimizing the indexes, moving from an average of 1500 Lucene segments post indexing to 24 (1 segment per replica). This aims both at improving the performances and removing completely the deleted documents we had after restarting the indexing post crash. When overwriting or deleting a document, Lucene does not actually delete it but flags it at \"deleted\" until an optimize is performed.",
            "Our optimize script is extremely simple, starting with the indexes that have the most important number of deleted documents to save space.",
            "#!/bin/sh HOST=$1 CURL_BIN=$(which curl) if [ -z \"$HOST\" ]; then echo \"Host is missing\" exit 1 fi if [ -z \"$CURL_BIN\" ]; then echo \"Curl binary is missing\" exit 1 fi for indice in $(${CURL_BIN} -XGET http://$ {HOST}:9200/_cat/indices | sort -rk 7 | awk '{print $3}'); do if [ ! -z \"$indice\" ]; then echo $(date +\"%Y%m%d %H:%M\") Processing indice ${indice} ${CURL_BIN} -XPOST http://$ {HOST}:9200/${indice}/_optimize?max_num_segments=1 echo fi done exit 0",
            "Reindexing a large Elasticsearch cluster with major data mode changes was quite interesting. It allowed us to push Elasticsearch and our hardware boundaries to reach a correct throughput. Yoko and Moulinette are now reusable for every Elasticsearch cluster we run at Synthesio, allowing reindexing within a same cluster or cross clusters."
        ]
    }
}

